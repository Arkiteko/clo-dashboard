import streamlit as st
import pandas as pd
from pathlib import Path
from src.etl import ETLPipeline
from src.utils import ingest_file
from src.config import get_warehouse_config, save_warehouse_config, WarehouseConfig


# Config
DATA_DIR = Path("data")
RAW_DIR = DATA_DIR / "0_raw"
STAGING_DIR = DATA_DIR / "1_staging"
STANDARD_DIR = DATA_DIR / "2_standard"
PUBLISHED_DIR = DATA_DIR / "3_published"

# Ensure dirs exist
RAW_DIR.mkdir(parents=True, exist_ok=True)
PUBLISHED_DIR.mkdir(parents=True, exist_ok=True)

st.set_page_config(page_title="CLO Warehouse Platform", layout="wide")

st.title("CLO Warehouse Platform")

tabs = st.tabs(["Global Portfolio", "Warehouse Analytics", "Tape Ingestion", "Admin Settings"])

# Load all published data globally for use in tabs
files = list(PUBLISHED_DIR.glob("*.parquet"))
df_all = pd.DataFrame()
from datetime import datetime

if files:
    dfs = []
    for f in files:
        d = pd.read_parquet(f)
        name = f.stem # e.g. 20230101_120000_Warehouse_Alpha
        
        parts = name.split("_")
        
        # Default fallback
        data_date = datetime.now()
        upload_ts = "000000"
        warehouse_name = "Unknown"
        
        # New Format: YYYYMMDD_HHMMSS_Name...
        if len(parts) >= 3 and len(parts[0])==8 and len(parts[1])==6:
             try:
                 data_date = datetime.strptime(parts[0], "%Y%m%d")
                 upload_ts = parts[1]
                 warehouse_name = "_".join(parts[2:]).replace(".parquet", "")
                 # Remove 'Tape' suffix if present (generated by dummy script)
                 warehouse_name = warehouse_name.replace("_Tape", "")
             except:
                 pass
        else:
             # Legacy/Fallback parsing (try to recover old files)
             try:
                 ts_str = parts[0]
                 if len(ts_str) == 8:
                      data_date = datetime.strptime(ts_str, "%Y%m%d")
                 if "Warehouse" in parts:
                     idx = parts.index("Warehouse")
                     warehouse_name = "_".join(parts[idx:idx+2])
             except:
                 pass

        d["warehouse_source"] = warehouse_name
        d["data_date"] = data_date
        d["upload_ts"] = upload_ts # useful for sorting
        d["data_date"] = pd.to_datetime(d["data_date"])
        
        dfs.append(d)
    
    if dfs:
        df_raw = pd.concat(dfs, ignore_index=True)
        # Deduplication Logic
        # Sort by Upload Timestamp ascending, then drop duplicates keeping last
        # This means if we have 2 files for 2023-01-01, we keep the one uploaded later.
        df_all = df_raw.sort_values("upload_ts").drop_duplicates(
             subset=["warehouse_source", "data_date", "asset_id"], # Granular dedup? No, usually replace entire tape.
             keep="last"
        )
        # Actually, typically we replace the ENTIRE DAY's tape. 
        # But here we are concatenating asset-level rows. 
        # If we just uploaded a new file for the SAME DATE, we probably want to ignore the old file's rows entirely.
        
        # Better approach: Filter at FILE level or bulk drop.
        # Let's do: Group keys (Warehouse, Date) -> find max TS. Filter source DFs.
        # But since we already loaded, let's just group by (Warehouse, Date) and keep rows from the max TS.
        
        # 1. Identify valid (Warehouse, Date, TS) tuples
        versions = df_raw[["warehouse_source", "data_date", "upload_ts"]].drop_duplicates()
        # 2. Find max TS for each (Warehouse, Date)
        latest_versions = versions.sort_values("upload_ts").groupby(["warehouse_source", "data_date"]).tail(1)
        
        # 3. Join back to keep only those rows
        # A simple merge on all 3 columns acts as a filter
        df_all = df_raw.merge(latest_versions, on=["warehouse_source", "data_date", "upload_ts"], how="inner")


with tabs[0]:
    st.header("Global Portfolio Overview")
    
    if df_all.empty:
        st.info("No published data found. Please ingest some tapes.")
    else:
        # Filter for LATEST snapshot per warehouse for the Overview Cards
        latest_indices = df_all.groupby("warehouse_source")["data_date"].idxmax()
        df_latest = df_all.loc[latest_indices].copy()
        
        # Enrich with Strategy Type from Config
        # We need to look up config for each filtered row
        def get_type(row):
            c = get_warehouse_config(row["warehouse_source"])
            return c.warehouse_type
            
        df_latest["Strategy"] = df_latest.apply(get_type, axis=1)
        
        # Overall Metrics (Latest)
        total_funded = df_latest.groupby("warehouse_source").apply(lambda x: x["par_amount"].sum()).sum()
        
        # W.Avg Price (Global across latest)
        global_par = df_latest["par_amount"].sum()
        weighted_avg_price = (df_latest["par_amount"] * df_latest["market_price"]).sum() / global_par if global_par > 0 else 0
        total_assets = len(df_latest)
        
        # Global WAS / WAL / CCC
        # Need to handle missing cols if old data, but we just regen'd
        if "spread" in df_latest.columns:
            weighted_avg_spread = (df_latest["par_amount"] * df_latest["spread"]).sum() / global_par if global_par > 0 else 0
        else:
            weighted_avg_spread = 0
            
        if "maturity_date" in df_latest.columns:
            today_date = pd.to_datetime(datetime.now())
            df_latest["years_to_mat"] = (df_latest["maturity_date"] - today_date).dt.days / 365.0
            weighted_avg_life = (df_latest["par_amount"] * df_latest["years_to_mat"]).sum() / global_par if global_par > 0 else 0
        else:
            weighted_avg_life = 0
            
        ccc_exposure = 0
        if "rating_moodys" in df_latest.columns:
             ccc_exposure = df_latest[df_latest["rating_moodys"].str.contains("Caa") | df_latest["rating_moodys"].str.contains("C")]["par_amount"].sum()
        ccc_pct = ccc_exposure / total_funded if total_funded > 0 else 0

        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Total Funded Exposure", f"${total_funded/1_000_000:,.1f}M")
        c2.metric("Portfolio W.Avg Price", f"{weighted_avg_price:.2f}")
        c3.metric("W.Avg Spread (WAS)", f"{weighted_avg_spread:.0f} bps")
        c4.metric("W.Avg Life (WAL)", f"{weighted_avg_life:.2f} yrs")
        
        c5, c6, c7, c8 = st.columns(4)
        c5.metric("Total Assets", total_assets)
        c6.metric("Active Warehouses", len(df_latest))
        c7.metric("Global CCC %", f"{ccc_pct:.2%}")
        
        st.divider()
        st.subheader("Exposure by Strategy")
        
        # Group by Strategy
        strat_stats = []
        if "Strategy" in df_latest.columns:
            for strat, g in df_latest.groupby("Strategy"):
                s_fund = g.groupby("warehouse_source").apply(lambda x: x["par_amount"].sum()).sum()
                s_price = (g["par_amount"] * g["market_price"]).sum() / s_fund if s_fund > 0 else 0
                s_was = (g["par_amount"] * g["spread"]).sum() / s_fund if s_fund > 0 else 0
                s_count = g["warehouse_source"].nunique()
                strat_stats.append({
                    "Strategy": strat,
                    "Funded Exposure": s_fund,
                    "W.Avg Price": s_price,
                    "WAS": s_was,
                    "Warehouses": s_count
                })
            
        if strat_stats:
            df_strat = pd.DataFrame(strat_stats)
            
            # Show Metrics Columns
            cols = st.columns(len(strat_stats))
            for idx, row in df_strat.iterrows():
                with cols[idx]:
                    st.metric(f"{row['Strategy']}", f"${row['Funded Exposure']/1e6:,.1f}M", f"WAS: {row['WAS']:.0f} bps")
                    
            # st.bar_chart(df_strat.set_index("Strategy")["Funded Exposure"])
        
        st.divider()
        
        col_charts1, col_charts2 = st.columns(2)
        
        with col_charts1:
            st.subheader("Global Industry Concentration")
            if "industry_gics" in df_latest.columns:
                ind_exp = df_latest.groupby("industry_gics")["par_amount"].sum().sort_values(ascending=False).head(10)
                st.bar_chart(ind_exp)
                
        with col_charts2:
            st.subheader("Global Rating Distribution")
            if "rating_moodys" in df_latest.columns:
                rtg_exp = df_latest.groupby("rating_moodys")["par_amount"].sum()
                # Sort?
                st.bar_chart(rtg_exp)
        
        st.subheader("Top 20 Issuers (Global)")
        if "issuer_name" in df_latest.columns:
            iss_exp = df_latest.groupby("issuer_name")["par_amount"].sum().sort_values(ascending=False).head(20)
            st.dataframe(iss_exp.reset_index().rename(columns={"par_amount": "Total Exposure"}), use_container_width=True)


with tabs[1]:
    st.header("Warehouse Analytics")
    
    if df_all.empty:
        st.info("No data available.")
    else:
        # Selector
        wh_list = sorted(df_all["warehouse_source"].unique())
        selected_wh = st.selectbox("Select Warehouse", wh_list, key="wh_select_analytics")
        
        # Full history for trends
        df_wh_history = df_all[df_all["warehouse_source"] == selected_wh].copy().sort_values("data_date")
        
        # Latest snapshot for current metrics
        latest_date = df_wh_history["data_date"].max()
        df_wh = df_wh_history[df_wh_history["data_date"] == latest_date].copy()
        
        config = get_warehouse_config(selected_wh)
        
        # Metrics Calculation (Latest)
        wh_funded = df_wh["par_amount"].sum()
        wh_price = (df_wh["par_amount"] * df_wh["market_price"]).sum() / wh_funded if wh_funded > 0 else 0
        
        # New Metrics: WAS and WAL
        # WAS = Weighted Average Spread
        wh_was = (df_wh["par_amount"] * df_wh["spread"]).sum() / wh_funded if wh_funded > 0 else 0
        
        # WAL = Weighted Average Life (Years from Today to Maturity)
        # simplistic day count / 365
        today_date = pd.to_datetime(latest_date)
        df_wh["years_to_mat"] = (df_wh["maturity_date"] - today_date).dt.days / 365.0
        wh_wal = (df_wh["par_amount"] * df_wh["years_to_mat"]).sum() / wh_funded if wh_funded > 0 else 0
        
        # Simulated Snapshot Inputs
        st.markdown(f"#### ðŸŸ¢ Compliance & Metrics (As of {latest_date.date()}) [{config.warehouse_type}]")
        
        # Calc Implied Debt if not entered
        implied_debt = wh_funded * (wh_price/100) * config.advance_rate
        
        # Inputs for Calc - these define the "Current State" for the snapshot
        c_snap1, c_snap2 = st.columns(2)
        debt_outstanding = c_snap1.number_input("Debt Outstanding (Snapshot)", value=implied_debt, help="Enter actual debt from report")
        cash_balance = c_snap2.number_input("Cash Balance", value=0.0)
        
        # OC Calculation
        total_collateral_par = wh_funded
        numerator = total_collateral_par + cash_balance
        denominator = debt_outstanding if debt_outstanding > 0 else 1.0
        
        current_oc = numerator / denominator
        
        # Display Key Metrics Row
        m1, m2, m3, m4 = st.columns(4)
        m1.metric("Facility Utilization", f"{(debt_outstanding / config.max_facility_amount)*100:.1f}%", f"Limit: ${config.max_facility_amount/1e6:.0f}M")
        
        oc_delta = current_oc - config.oc_trigger_pct
        m2.metric("OC Ratio", f"{current_oc:.2%}", f"{oc_delta:.2%} vs Trig ({config.oc_trigger_pct:.0%})", delta_color="normal" if oc_delta >= 0 else "inverse")
        
        m3.metric("Equity (NAV)", f"${(numerator - debt_outstanding)/1e6:,.2f}M")
        m4.metric("W.Avg Price", f"{wh_price:.2f}")

        # Secondary Metrics Row
        m5, m6, m7, m8 = st.columns(4)
        m5.metric("W.Avg Spread (WAS)", f"{wh_was:.0f} bps")
        m6.metric("W.Avg Life (WAL)", f"{wh_wal:.2f} yrs")
        
        # Calc CCC
        ccc_exposure = df_wh[df_wh["rating_moodys"].str.contains("Caa") | df_wh["rating_moodys"].str.contains("C")]["par_amount"].sum()
        ccc_pct = ccc_exposure / wh_funded if wh_funded > 0 else 0
        m7.metric("CCC Exposure", f"{ccc_pct:.1%}", "Limit: 7.5%", delta_color="inverse" if ccc_pct > 0.075 else "normal")
        
        m8.metric("Asset Count", len(df_wh))

        st.divider()
        
        # Tabs for detailed analysis
        subtabs = st.tabs(["Asset Quality", "Trends", "Portfolio Composition"])
        
        with subtabs[0]:
            st.subheader("Credit Quality")
            
            c_qual1, c_qual2 = st.columns(2)
            
            with c_qual1:
                st.markdown("**Rating Distribution**")
                if "rating_moodys" in df_wh.columns:
                    # Sort ratings logic? simpler to just count for now
                    rtg_exp = df_wh.groupby("rating_moodys")["par_amount"].sum()
                    st.bar_chart(rtg_exp)
            
            with c_qual2:
                st.markdown("**Rating Migration (vs Original)**")
                if "original_rating_moodys" in df_wh.columns:
                    # Simple table
                    df_mig = df_wh[["asset_id", "issuer_name", "original_rating_moodys", "rating_moodys", "par_amount"]].copy()
                    df_mig["Downgraded"] = df_mig.apply(lambda x: x["rating_moodys"] != x["original_rating_moodys"], axis=1) # naive check
                    st.dataframe(df_mig[df_mig["Downgraded"]], hide_index=True)
                    
        with subtabs[1]:
            st.subheader("Historical Trends")
            # Prepare Trend Data
            trend_data = []
            for d, g in df_wh_history.groupby("data_date"):
                 fnd = g["par_amount"].sum()
                 px = (g["par_amount"] * g["market_price"]).sum() / fnd if fnd > 0 else 0
                 
                 # Estimate simple OC
                 est_debt = fnd * (px/100) * config.advance_rate
                 est_oc = fnd / est_debt if est_debt > 0 else 0
                 
                 trend_data.append({
                     "Date": d,
                     "Funded Exposure": fnd,
                     "W.Avg Price": px,
                     "Est. OC%": est_oc
                 })
            
            df_trend = pd.DataFrame(trend_data).set_index("Date")
            
            t1, t2 = st.columns(2)
            with t1:
                st.line_chart(df_trend["Funded Exposure"])
            with t2:
                st.line_chart(df_trend["W.Avg Price"])

        with subtabs[2]:
            st.subheader("Concentration Analysis")
            
            c_conc1, c_conc2 = st.columns(2)
            with c_conc1:
                 st.write("**Top Industries**")
                 ind_exp = df_wh.groupby("industry_gics")["par_amount"].sum().sort_values(ascending=False).head(5)
                 st.bar_chart(ind_exp)
                 
            with c_conc2:
                 st.write("**Top Obligors**")
                 iss_exp = df_wh.groupby("issuer_name")["par_amount"].sum().sort_values(ascending=False).head(5)
                 st.bar_chart(iss_exp)
            
            st.markdown("---")
            st.dataframe(df_wh[["asset_id", "issuer_name", "par_amount", "market_price", "spread", "maturity_date", "rating_moodys"]], use_container_width=True)



with tabs[2]:
    st.markdown("### Data Tape Ingestion & Validation")

    uploaded_file = st.file_uploader("Drop Standard Template (Excel)", type=["xlsx"])
    warehouse_name = st.selectbox("Select Warehouse Name for Ingestion", ["Warehouse_Alpha", "Warehouse_Beta", "Warehouse_Gamma"])
    as_of_date = st.date_input("As Of Date")
    
    process_btn = st.button("Process Tape")

    if uploaded_file and process_btn:
        with st.spinner("Ingesting and Validating..."):
            # 1. Ingest
            raw_path = ingest_file(uploaded_file, RAW_DIR, warehouse_name, as_of_date)
            st.success(f"File saved: {raw_path.name}")
            
            # 2. Run ETL
            pipeline = ETLPipeline(RAW_DIR, STAGING_DIR, STANDARD_DIR)
            df, issues = pipeline.process_tape(raw_path)
            
            if df is not None:
                # Display Results
                st.divider()
                st.subheader("Validation Report")
                
                hard_errors = [i for i in issues if i.severity == "HARD"]
                soft_warns = [i for i in issues if i.severity == "SOFT"]
                
                c1, c2 = st.columns(2)
                c1.metric("Hard Errors", len(hard_errors), delta_color="inverse" if hard_errors else "off")
                c2.metric("Warnings", len(soft_warns), delta_color="normal")
                
                if hard_errors:
                    st.error("Blocking Issues Found - Data NOT Published:")
                    for e in hard_errors:
                        st.write(f"- {e.message} (Row IDs: {e.row_id})")
                
                if soft_warns:
                    st.warning("Warnings:")
                    for w in soft_warns:
                         st.write(f"- {w.message}")
                         
                if not hard_errors:
                    st.success("Published Successfully!")
                    st.cache_data.clear()
            else:
                st.error("Failed to parse file.")

    st.divider()
    st.markdown("### Load History")
    
    if df_all.empty:
        st.info("No load history.")
    else:
        # Show what is currently loaded/active
        # We can also show 'Overwritten' files if we scanned the raw dir, but showing the Active set is good for now.
        # Actually user wants 'History' so let's show the deduplicated active view + maybe a count of versions?
        
        # Simple table of the ACTIVE dataset
        hist_df = df_all.groupby(["warehouse_source", "data_date"]).agg(
            Assets=("asset_id", "count"),
            Total_Funded=("par_amount", "sum"),
            Last_Updated=("data_date", "max") # Placeholder, really want upload time
        ).reset_index().sort_values(["warehouse_source", "data_date"], ascending=[True, False])
        
        # Format for display
        hist_df["Total_Funded"] = hist_df["Total_Funded"].apply(lambda x: f"${x/1e6:,.2f}M")
        
        st.dataframe(hist_df, use_container_width=True)


with tabs[3]:
    st.header("Admin Settings")
    st.info("Configure Limits and Triggers for each warehouse here.")
    
    if df_all.empty:
         st.warning("No warehouses found. Please ingest data first.")
    else:
        wh_list = sorted(df_all["warehouse_source"].unique())
        selected_wh_admin = st.selectbox("Select Warehouse to Configure", wh_list, key="wh_select_admin")
        
        config = get_warehouse_config(selected_wh_admin)
        
        with st.form("admin_config_form"):
            st.subheader(f"Configuration for: {selected_wh_admin}")
            
            # Type Selector
            new_type = st.selectbox("Warehouse Type", ["BSL", "Middle Market"], index=0 if config.warehouse_type=="BSL" else 1)
            
            c_adm1, c_adm2 = st.columns(2)
            
            with c_adm1:
                st.markdown("#### Facility Limits")
                new_max = st.number_input("Max Facility Size ($)", value=float(config.max_facility_amount))
                new_adv = st.slider("Advance Rate", 0.0, 1.0, value=float(config.advance_rate))
            
            with c_adm2:
                st.markdown("#### Compliance Triggers")
                new_oc = st.number_input("Min OC Ratio Trigger (decimal)", value=float(config.oc_trigger_pct), step=0.01, help="e.g. 1.25 for 125%")
                new_conc = st.slider("Max Industry Concentration", 0.0, 1.0, value=float(config.concentration_limit_industry))
            
            if st.form_submit_button("Save Configuration"):
                new_cfg = WarehouseConfig(
                    max_facility_amount=new_max,
                    advance_rate=new_adv,
                    oc_trigger_pct=new_oc,
                    concentration_limit_industry=new_conc,
                    warehouse_type=new_type
                )
                save_warehouse_config(selected_wh_admin, new_cfg)
                st.success(f"Settings saved for {selected_wh_admin}")
                st.rerun()
